\chapter{ОПИСАНИЕ МЕТОДА}

Метод регуляризации подробно изложен в статье \cite{turchin}. Текст оригинальной работы достаточно сложно найти, поэтому мы изложим в этом обзоре основные положения методики

\section{Стратегия}
Статистический метод регуляризации основан на Байессовой статистике и методе принятии решений в условии неопределенности. Сначала необходимо перейти из непрерывного пространства функций в параметризованное дискретное представление. Такой переход называется алгебраизацией. В результате функции $f$ и $\varphi$ превращаются в вектора, а оператор $\hat{K}$ в матрицу. В результате уравнение (\ref{eq:opereq}) преобразуется в
\begin{equation}
  f_m = K_{mn}\varphi_n,
\label{eq:algebr}
\end{equation}
где $f_m$ --- набор (вектор) случайных величин с известной плотностью вероятности, представляющих собой результат измерения в конечном числе $M$ точек $y_m$ на отрезке $[c;d]$; $\varphi_n$ --- вектор, задающий алгебраизацию исследуемого закона природы (функции $\varphi$), $K_{mn}$ --- известная матрица, задающая преобразование $\vec{\varphi}$ в  $\vec{f}$.
В результате исходную задачу можно переформулировать следующим образом: по известной реализации $\vec{f}$ нам нужно оценить значение параметра $\vec{\varphi}$. Функцию $\vec{S}$, которая определяется как алгоритм нахождения оценок $\vec{\varphi}$ на основе $\vec{f}$, мы будем называть стратегией. 

В общем случае форма произвольной функции описывается бесконечномерным вектором, поэтому описание при помощи вектора конечной размерности приводит к частичной потере информации. Методика никак не ограничивает набор базисных функций, по которым производится разложение, но выбор конкретных параметров алгебраизации существенно влияет на результат работы. Примеры различных способов алгебраизации будут рассмотрены в ниже.

Для сравнения результатов, полученных с помощью разных стратегий, вводится \textit{квадратичная функция потерь}:

\begin{equation}
	L(\hat{\vec{\varphi}},\vec{S}) = \sum\mu_n(\hat{\varphi}_n-S_n)^2,
\end{equation}
где $\hat{\vec{\varphi}}$ --- наилучшее решение, а $\mu_n$ --- весовой коэффициент. Тогда для некоторого решения $\varphi$ потери выбранной нами стратегии задаются \textit{функцией риска}:

\begin{equation}
	R_{\vec{S}}(\vec{\varphi}) \equiv E[L(\vec{\varphi},\vec{S})] = \int L(\vec{\varphi},\vec{S})P(\vec{f}|\vec{\varphi})d\vec{f}, 
\end{equation}
где $P(\vec{f}|\vec{\varphi})$ --- это плотность вероятности ансамбля, по которому производится усреднение потерь. Этот ансамбль образован гипотетическим многократным повторением  измерений $\vec{f}$ при заданном $\vec{\varphi}$, таким образом $P(\vec{f}|\vec{\varphi})$ это та самая известная нам плотность вероятности $\vec{f}$, полученная в эксперименте.

Согласно Байессовскому подходу предлагается рассмотреть $\vec{\varphi}$, как \textbf{случайную переменную} с \textit{априорной плотностью вероятности} $P(\vec{\varphi})$, выражающую \textbf{достоверность} различных возможных законов природы. $P(\vec{\varphi})$ определяется на основе информации, существующей до проведения эксперимента \cite{chernov}. Тогда выбор оптимальной стратегии основывается на минимизации \textit{апостериорного риска}:
\begin{equation}
	r_{\vec{S}}(\vec{\varphi}) \equiv E_{\vec{\varphi}}E_{\vec{f}}[L(\vec{\varphi},\vec{S})|\vec{\varphi}].
\end{equation}
В этом случае оптимальная стратегия хорошо известна:
\begin{equation}
	\label{eq:opt}
	\vec{S}_{opt} = E[\vec{\varphi}|\vec{f}] =\int \varphi_n P(\vec{\varphi}|\vec{f})d\vec{\varphi},
\end{equation}

\noindent  где \textit{апостерионая плотность} $P(\vec{\varphi}|\vec{f})$ определяется по теореме Баейса:

\begin{equation}
P(\vec{\varphi}|\vec{f})= \frac{P(\vec{\varphi})P(\vec{f}|\vec{\varphi})}{\int d\vec{\varphi}P(\vec{\varphi})P(\vec{f}|\vec{\varphi})} .
\end{equation}

\noindent Кроме того, такой подход позволит определить дисперсию полученного решения:

\begin{equation}
\left\langle \sigma_n^2 \right\rangle = \int (\varphi_n - S^{opt}_n)^2 P(\vec{\varphi}|\vec{f})d\vec{\varphi}.
\end{equation}

Итак, мы получили оптимальное решение уравнения (\ref{eq:opereq}), введя априорную плотность $P(\vec{\varphi})$. Метод статистической регуляризации основан на введении априорной информации о $\varphi(x)$. Если исследователь уже обладает какой-либо априорной информацией (априорной плотностью $P(\vec{\varphi})$, он может просто посчитать интегралы (\ref{eq:opt}) и получить ответ. Для случая, если такой информации нет, в следующем параграфе описывается, какой минимальной информацией может обладать исследователь и как её использовать для получения регуляризованного решения.

\section{Априорная информация}

При работе с Байесовской статистикой ключевым вопросом всегда является выбор априорной вероятности, специфичной для конкретной задачи. Наиболее часто встречающееся в физике ограничение на вид исследуемых функций - это требование непрерывности и гладкости, причем под гладкостью подразумевается не только наличие непрерывной второй производной, но и ее сравнительно маленькое значение. Это связано с тем, что в физических процессах как правило не бывает резких перепадов. Требование минимальной гладкости является очевидным выбором для априорной информации. Поскольку конкретное ограничение на значение второй производной обычно неизвестно, можно также потребовать, чтобы дополнительная информация, возникающая в результате ограничения была минимальной.

Эту задачу можно формализовать следующим образом: требуется найти $P(\vec{\varphi})$, при котором функционал информации Шеннона:

\begin{equation}
	\label{eq:inforamation}
	I[P(\vec{\varphi})] = \int \ln{P(\vec{\varphi})} P(\vec{\varphi}) d\vec{\varphi}
\end{equation}
минимален, и при этом выполнялись следующие условия:
\begin{enumerate}
	\item Плотность вероятности нормированна на единицу:
	\begin{equation}
		\int P(\vec{\varphi}) d\vec{\varphi} = 1
	\end{equation}
	
	\item Условие на гладкость $\varphi(x)$. Пусть $\Omega$ --- некоторая матрица характеризующая гладкость функции. Тогда потребуем, чтобы достигалось определённое значение функционала гладкости:
	\begin{equation}
		\label{eq:glad}
		\int (\vec{\varphi},\Omega\vec{\varphi}) P(\vec{\varphi}) d\vec{\varphi} = \omega,
	\end{equation}
	Обратим внимание на два обстоятельства. Во-первых, значение параметра $\omega$, скорее всего, неизвестно, и способы его определения будут рассмотрены далее в обзоре. Во-вторых, мы уже неявно использовали априорную информацию о гладкости функции, когда определяли функцию полезности: квадратичная функция полезности является хорошим выбором, потому что любая регулярная функция, вблизи экстремума хорошо приближается квадратичной формой.
\end{enumerate}

Эта задача решена в \cite{turchin}, решение приведено в приложении \ref{sec:metod}. В результате преобразований получается следующее решение:

\begin{equation}
	\label{eq:apriori}
	P_{\alpha}(\vec{\varphi})  = \frac{\alpha^{Rg(\Omega)/2}\det\Omega^{1/2}}{(2\pi)^{N/2}} \exp(-\frac{1}{2} (\vec{\varphi},\alpha\Omega\vec{\varphi})),
\end{equation}
где $\alpha = 1/\omega$. 
Значение параметра $\alpha$ на этом этапе неизвестно, и может быть получено следующими способами:

\begin{itemize}
	\item напрямую из каких-то внешних данных или подобрано вручную (в этом частном случае результаты работы метода эквиваленты регуляризации Тихонова \cite{tihonov},
	\item как максимум апостериорной информации $P(\alpha|\vec{f})$,
	\item как среднее по всем возможным $\alpha$, определив априорную плотность вероятности как:
		\begin{equation}
			P(\vec{\varphi}) = \int d\alpha P(\alpha) P(\vec{\varphi}|\alpha),
		\end{equation}
		причем в соответствии с байесовским подходом, можно принять все $\alpha$ равновероятными.
\end{itemize}

\section{Случай гауссовых шумов}

Наиболее распространенным в экспериментальной физике является случай, когда разброс результатов эксперимента подчиняется нормальному распределению. В этом случае регуляризация имеет аналитическое решение.
Пусть вектор измерений $f$ имеет ошибки, описываемые многомерным гауссовым распределением с ковариационной матрицей $\Sigma$:

\begin{equation}
	P(\vec{f}|\vec{\varphi}) = \frac{1}{(2\pi)^{M/2}|\Sigma|^{1/2}} \exp(-\frac{1}{2}(\vec{f} - K\vec{\varphi})^T\Sigma^{-1}(\vec{f} - K\vec{\varphi}))
	\label{eq:gaussP}
\end{equation}

Будем искать оценку $\alpha$, как наиболее вероятное по апостериорной плотности вероятности $P(\alpha|\vec{f})$. В \cite{turchin} показано, что для этого нужно взять $\alpha*$, при котором достигается максимум функции: 

\begin{equation}
	\label{eq:alphamax}
	F(\alpha,\vec{f}) = \frac{Rg(\Omega)}{2}\ln{\alpha} - \frac{1}{2}\ln{|B+\alpha\Omega|}  + \frac{1}{2}b^{T}(B+\alpha\Omega)^{-1}b
\end{equation}
Такой максимум должен существовать, к тому же, при наличии достоточной информации для существования разумного решения, дисперсия $P(\alpha|f)$ не должна быть слишком большая. Тогда подставив найденное значение наиболее вероятного $\alpha^*$ в выражение для апостериорной вероятности $\vec{\varphi}$:  
\begin{comment}Вообще говоря, наша выборка $\vec{f}$ не обязана содержать достаточно информации для существования наиболее вероятного $\alpha^*$. При обработке данных следует проверить этот факт, исследовав функцию \begin{equation}
	\label{eq:alphaaposter}
	P(\alpha|\vec{f}) = C \alpha^{\frac{Rg(\Omega)}{2}}\sqrt{|(B+\alpha\Omega)^{-1}|}\exp(-\frac{1}{2}b^{T}B^{-1}b)\exp(\frac{1}{2}b^{T}(B+\alpha\Omega)^{-1}b)
\end{equation}
Если же наше предположение оправдано, то подставим полученное $\alpha^*$ в выражение для апостериорной вероятности $\vec{\varphi}$:
\end{comment}

\begin{gather*}
	P(\vec{\varphi}|\vec{f})= \frac{P(\vec{\varphi})P(\vec{f}|\vec{\varphi})}{\int d\vec{\varphi}P(\vec{\varphi})P(\vec{f}|\vec{\varphi})} = \\
	=\frac{1}{(2\pi)^{N/2}|(K^T\Sigma^{-1}K+\alpha^*\Omega)^{-1}|^{1/2}} \times \\
	\times \exp(-\frac{1}{2} (\vec{\varphi},(K^T\Sigma^{-1}K + \alpha^*\Omega)\vec{\varphi}) + K^T\Sigma^{-1T}\vec{f}\vec{\varphi})
\end{gather*}
Поскольку апостериорная плотность вероятности получилась гауссовой, а наилучшее решение является математическим ожидание данного распределения, не составляет труда выписать это решение и его ковариационную матрицу (по которой можно определить ошибки, согласно приложению ~\ref{sec:gauss}):

\begin{equation} \label{eq:analit_solv}
	\vec{S}_{opt} = (K^T\Sigma^{-1}K+\alpha^*\Omega)^{-1}K^T\Sigma^{-1T}\vec{f}
\end{equation}

\begin{equation}
	\texttt{cov}(\varphi_m, \varphi_n) = ||(K^T\Sigma^{-1}K+\alpha^*\Omega)^{-1}||_{mn}
\end{equation}
